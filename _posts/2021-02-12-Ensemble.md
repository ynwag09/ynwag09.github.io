---
layout : posts

title : Ensemble model

categories :
  - Ensemble

tag : 
  - ensemble

use_math : ture
---
  - Idea
    
    ## add Diversity! 


  - 종류
    ![diff](../images/ensemble/bagging_boosting.png)
    - bagging 
        - use bootstraped(*with replacement* ) B dataset for training and modeling (Not K-fold)
        -  naturally get OOB(out of bag) data, which is differ each bag, for testing
        $ p  = \bigg( (1 - \frac{1}{N})^N \bigg) → \displaystyle\lim_{N\to \infty}(1 - \frac{1}{N})^N = e^{-1} \approx 1/3$
        - suit for low bias, high variance model
            ex) svm, neural, 

        - result aggregation 
            - classification : major voting , weighted voting (by accuracy, probability)

            - stacking : make another model (input : results of each bags / output : true y-value)
            ![stacking](../images/ensemble/stacking.png)
    - boosting
        - 이후 post에서 정리함  
    
    - recursive한 boosting과 달린 bagging은 병렬처리가 가능하지만
    한 model을 fit하는데 시간이 오래걸리기 때문에 산술적 비교는 불가능하다.


  ## RF

  - use bootstrap and variable selection when branching
  - feature importance (OOB data로 계산)
      M개의 tree가 있음
      i번째 변수의 importance를 계산하려면
     1) $e_{m}$ = m번째 oob error 
     2) $p_{m,i}$ = i번째 변수를 random permute한 후의 m번째 oob error
     3) $d_{i}^{m} = e_m - p_{m,i}$ 
     4) $\bar{d_i} = \frac{1}{M}\displaystyle\sum_{m=1}^{M}d_{i}^{m}$
        $s_{i}^2 = var(d_i)$
     5) $v_i = \frac{\bar{d_i}}{s_{i}^2}$

     feature importance is relative value