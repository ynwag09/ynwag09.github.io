---
layout : posts
title : "UMAP"

categories : 
  - Dimension reduction

tag : 
  - Umap
  - Dimension reduction

use_math : ture

comments: true
---





### Uniform Manifold Approximation and Projection

아래의 글은 umap을 공부하면서 개인적으로 정리한 내용들입니다. 

umap은 2018년에 나온 차원축소 (dimension reduction technique) 입니다.

umap은 data에 대한 몇 가지 가정들을 통해 이전에 존재하던 차원축소 방법들(PCA, t-SNE)보다 좋은 performance를 보여줍니다.

umap의 가정들이 무엇인지 확인해보고 이를 적용한 알고리즘의 작동방식을 알아보겠습니다.



## 1. 배경지식 : manifold learning, TDA 

umap의 이름에서도 알 수 있듯이 umap은 manifold learning 이다. 

또한 umap의 idea는 topological data analysis를 기반으로 뒀기때문에 이 둘의 개념을 알아야 한다. 



 ### Manifold

> a manifold is a second countable Hausdorff space that is locally homeomorphic to Euclidea space.
>
> Informally, a manifold is a space that is "modeled on" Euclidean space.

![manifold](https://upload.wikimedia.org/wikipedia/commons/thumb/6/64/Circle_with_overlapping_manifold_charts.svg/1920px-Circle_with_overlapping_manifold_charts.svg.png){: width="40%" height="40%"}		

정확한 정의를 완벽히 이해하면 좋겠지만

manifold란 무엇이냐고 물어보면 locally Eucildean space를 갖는 이상한 모양체라고 답하면 된다.

곡선을 아주 확대해서 보면 직선처럼 보이는것처럼 아무리 이상하게 생긴 모양체라도 아주 확대해서보면 euclidean distance가 적용되는 평면처럼 보인다는 것이다. (지구는 둥글지만 우리가 서있는 땅은 평평해보인다.)

manifold learning은 고차원(D-dim)의 데이터가 p<<D 인 저차원의(p-dim) manifold의 형태로 존재하여 

이 형태를 unfold 해 데이터에 있는 유의미한 정보들을 얻는 방법이다.

[link](https://bjlkeng.github.io/posts/manifolds/ "link") : manifold에 대한 좋은 정리. 후에 '리만 기하'에 대한 힌트도 얻을 수 있다. 

![manifold_learning](https://prateekvjoshi.files.wordpress.com/2014/06/3-swissroll-unfolded.png){: width : "80%" height = "80%"}

### TDA (Topological Data Analysis)

Topology를 데이터 분석에 접목시킨 것이다. 

사실 manifold도 locally euclidean인 topological space이다.

toplogical space 는 (X, $\tau$) 로 정의되는데 확률론의 algebra와 개념이 비슷하다.

Topology는 나도 수학과가 아니기에 정확히 모르고 umap에 사용된 몇 가지 개념들, 주로 graph theory,을 정리했다.

[link](https://www.youtube.com/watch?v=P_L3VJp-a2g&list=PLDZ6LA16SDbIvbgmCjcCuTA7mttfXjiec "link") : TDA에 대한 좋은 강의

 1. Simplex 

    > Simplex is a generalization of notion of a triangle or tetrahedron to arbitrary dimensions.

    ![simplex](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e1/Simplexes.jpg/542px-Simplexes.jpg)

    K-simplex는 k+1 vertices의 convex-hull이다. 직관적으로 그림을 보면 무엇인지 알 수 있다.

    graph와 연관지어 생각하면 K-simplex는 k+1개의 vertices가 모두 이어진 complete graph이다.

    > A complete graph is a graph whose vertices are all connected

    - face of simplex($\sigma$)

      $\tau$ : consecutive boundary of simplex($\sigma$) such that $\tau$ $\leq$ $\sigma$

      proper face satisfies $\tau$ < $\sigma$ 

      예를들어 3-simplex의 face는 {각 vertices, vertices들을 잇는 edges, edges들로 만들어진 planes, 자기자신} 이다.

	2. Simplical complex

    > a simplical complex $K$ is a set of simplices that satisfies the following conditions :
    >
    > 1. Every face of a simplex form $K$ is also in $K$ 
    > 2. The non-empty intersection of any two simplices $\sigma_1$, $\sigma_2$$\in$ $K$ is either empty or face of both.
    >
    > ![simplical_complex](https://www.programmersought.com/images/881/1905b22e39f2d6b94aa571e4196370c1.png){: .align-left} 
    >
    > 오른쪽을 보면 두 simplices간의 intersection이 $K$ 에 속하지 않으므로 simplicial complex가 아니다.
    >
    > 직관적으로 보면 simplical complex는 simplexes들을 vertices에서만 결합한 것이다. 

	3. Vietoris-Rips complex & Cech complex

    두 complex는 simplical complex의 special case이다.  

    $r$ 이 주어졌을때 각 vertices마다 r을 반지름으로 하는 원을 그려, 두 원이 만나면 edge가 생긴다.

    단 세점의 경우 Vietoris-Rips complex는 pairewise intersection만 있어도 2-simplex가 생기지만

    Cech complex는 non empty intersection일 때문 2-simplex가 생긴다.

    ![](https://d3i71xaburhd42.cloudfront.net/f8f36315d34d90c3ea788b033c85c0332c27c832/63-Figure4.3-1.png)

위의 내용을 이해했다면 

simplical complex는 점, 선, 면과 같은 간단한 요소들로 topological spaces를 생성하는 것을 알 수 있다. 

umap은 이런 TDA의 개념을 실제 데이터 분석에 적용한다. 이에 대한 이론적인 foundation은 Nerve theorem이다.

> the **nerve of an open covering** is a construction of an [abstract simplicial complex](https://en.wikipedia.org/wiki/Abstract_simplicial_complex) from an [open covering](https://en.wikipedia.org/wiki/Cover_(topology)#open_cover) of a [topological space](https://en.wikipedia.org/wiki/Topological_space) *X* that captures many of the interesting topological properties in an algorithmic or combinatorial way.

Nerve theorem은  simplcal complex로 구축된 topological space가 기존 topology의 대부분을 cover한다는 theorem이다.

umap에서는 cech complex를 사용하는것처럼 보인다.

![](https://umap-learn.readthedocs.io/en/latest/_images/how_umap_works_raw_data.png){: .align-center}

실제 데이터가 위와같이 주어져있다고 할때, radius $r$ 을 각 data point 마다 그리면 아래와 같다.

![](https://umap-learn.readthedocs.io/en/latest/_images/how_umap_works_open_cover.png){: .align-center}

이를 바탕으로 cech-complex를 만들면, 

![](https://umap-learn.readthedocs.io/en/latest/_images/how_umap_works_basic_graph.png){: .align-center}

이다. 즉 $r$ 을 적절하게 고르면 대부분 0,1,2-simplies 로 'glued' 된 cech-complex로 data를 represent 할 수 있다.

## 2. Model assumption 

계속해서 simplical complex로 구성된 data를 가공해보자. 

위에서 적절한 $r$ 에 대한 기준을 세우는 것이 첫번째이다. 

$r$ 이 너무 작으면 disconnection이 너무 많을 것이고, 

$r$ 이 너무 크면 대부분의 vertices들이 이어져 underlying manifold를 대표하지 못할 것이다.

이상적인 상황은 아래 사진처럼 모든 data point들이 manifold 위에서 uniformly distributed 한 상황일 것이다.

![](https://umap-learn.readthedocs.io/en/latest/_images/how_umap_works_uniform_distribution_cover.png){: .align-center}

하지만 실제 데이터가 이렇지 못하다는 것은 자명하다. 

umap 개발진들은 이를 간단히 다음과 같은 첫번째 가정을 통해 해결한다.

> The data is uniformly distributed on Riemnnian manifold.

data들이 위치한 manifold가 Riemnnian manifold라 가정하는 것이다.

이를 통해 얻을 수 있는 효과는 상당히 dramatic하고 반직관적이다.

각 점마다의 고정된 $r$ 을 사용하는 대신, 점 마다 다른 $r$ 을 사용한다.

$r_i$ 는 point마다 k번째 nearest neighbor까지의 거리이다. 

![](https://umap-learn.readthedocs.io/en/latest/_images/how_umap_works_local_metric_open_cover.png){: .align-center}

위와 같이 점마다의 눈에 보이는 radius는 다르지만, Riemann manifold상에서는 모두 같은 거리이다!

---

![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FFtoDj%2FbtqLzW5iNB5%2FOK8b9H2xmv1D9A9rXNKgU0%2Fimg.png)

Riemann manifold를 완전히 이해하는것은 어려웠지만 대강의 이해를 서술해본다.

위에서 정의했듯이 manifold는 locally Euclidean인 topological space이다. 따라서 manifold위의 모든점 마다 open neighborhood를 잡아 $\mathbb{R^n}$ 으로 embedding할 수 있다는 것이고 이 embedding 함수가 $\rho$ 이다.

큰 특징은 이 embedding이 유일하지 않다는 것이다. 또한 위에 그림에서 알 수 있듯이 manifold위에서 open neighborhood 사이의 intersection이 있을 경우, 다른 embedded space에서 일대일대응된다. 

따라서 각 embedding 공간의 basis가 다를 경우, 정의 되는 distance metric 역시 다를 수 있다. 매우 직관적인 예시이지만, 개미들이 보는 나뭇가지와 인간이 보는 나뭇가지의 절대적 길이는 똑같지만 각자의 길이에 대한 기준으로는 다르다. Riemann metric에서 나타나는 이러한 상대성이 후에 아인슈타인의 상대성이론의 토대가 되었다고 한다.

---

이렇게 uniformly distributed에 대한 가정을 한 뒤에는 적절한 $r$ 에 대한 고민이 적절한 neighborhood $k$ 로 전이된다. $k$ 를 선택하는 것이 $r$ 을 선택하는 것보다 좋은 점이 몇가지 있다. 우선 적절한 value를 고르는 것이 상대적으로 쉽다. $r$ 은 개별 point마다 다르지만 $k$ 의 경우 거시적인 관점에서 고를 수 있다. 둘째로 $k$ 는 해석상 의미를 갖는다. 높은 $k$ 를 고를 경우 global한 structure를, 낮은 $k$ 를 고를 경우 local한 structure를 표현한다고 볼 수 있다.

Riemann manifold에서의 가정으로 얻는 부수적이지만 어떻게 보면 중요한 영향이 한가지 더 있다. 이는 뒤에 나타나는 Fuzzy topology와 관련돼있다. 각 점 마다 varying distance를 갖으므로 각 점을 기준으로한 원을 확률적으로 표현할 수 있다. 즉 중심에 가까울 수록 나타날 확률이 높고, 반대로 멀 수록 나타날 확률이 낮은 것이다. 

![](https://umap-learn.readthedocs.io/en/latest/_images/how_umap_works_fuzzy_open_cover.png){: .align-center}

여기까지가 첫번째 가정으로 전개된 내용이다. 

다음으로 생각해야할 점은 high dimensional data의 경우 curse of dimension 때문에 모든 점들이 비슷한 크기의 거리로 멀리 떨어져 있다는 것이다. 즉 가까운 점이 없고 모든 점이 고립돼있다.

여기서 두번째 가정이 나타난다.

> all data points are locally connected. 

즉 각 점마다 적어도 nearest neighbor와는 neighborhood를 형성한다는 것이다. d(p, $k_1$) $\approx$ d(p, $k_{10}$) 이지만 d(p, $k_1$) 을 기준으로 삼고 다른 점들과의 거리를 보면 차이를 보인다는 것인데, 말로만 들으면 쉽게 이해하기는 힘들다. 이를 알고리듬의 수식으로 나타내보면 
$$
\rho_i  = min\{ d(x_i, x_{i_j})| 1\leq j \leq k , d(x_i, x_{i_j}) > 0 )\} \\
\sum_{j=1}^{K} \exp(\frac{-\max(0, d(x_i, x_{i_j}) - \rho_i)}{\sigma_i}) = \log_2k \\
w((x_i, x_{i_j})) = \exp(\frac{d(x_i, x_{i_j}) - \rho_i}{\sigma_i})
$$
$\rho_i$ 는 i번째 point가 가장 가까운 점과의 거리.

$\sigma_i$ 는 주어진 k(number of neighbor)에 대해 준식을 만족하는 값으로 point마다 값이 다르다.

graph의 edge value이 weight를 잘 보면 가장 가까운점보다 얼마나 먼지를 기준으로 exponetially 감소한다. 

즉 가장 가까운 점과의 weight가 1이므로 local connectivity가 의미하는 바를 알 수 있다.

![](https://umap-learn.readthedocs.io/en/latest/_images/how_umap_works_umap_open_cover.png){: .align-center}

이를 바탕으로 graph를 그리면 아래와 같다.

![](https://umap-learn.readthedocs.io/en/latest/_images/how_umap_works_raw_graph.png){: .align-center}

이 graph가 Riemann manifold와 local connectivity 가정으로 만든 graph이다. 문제점은 vertices 쌍마다 weight의 크기가 다르다. 이는 Riemann manifold에서 local embedding space의 local distance metric이 다르기 때문에 나타난다. 이를 간단하게 해결해주는 것이 fuzzy이론이다. 

위에서 했던 가정들을 통해 우리가 construct한 simplical complex로 이뤄진 manifold를 fuzzy topological structure로 modeling 할 수 있다. (fuzzy 이론에 대한 설명은 생략한다. 간단히 설명하자면 특정 point가 set A, set B에 배타적으로 속하는 것이 아니라 각각 일정한 확률로 속할 수 있다는 것이다.) 

> fuzzy union = p + q - p$*$q

![](https://umap-learn.readthedocs.io/en/latest/_images/how_umap_works_umap_graph.png){: .align-center}

이를 통해 final graph를 만들었다.

## 3. Finding low dimensional representation

이때까지 한 것은 주어진 high-dimensional data가 underlying manifold에 있다고 가정하고 manifold를 mathmatical assumption을 통해 fuzzy topological structure로 나타낸것이다. 이제 이 structure의 특징을 보존하도록 low dimension으로 projection해야한다. 

graph의 weight를 각 simplex가 존재할 확률로 본다면 low dimension으로의 근사의 loss function을 binary cross entropy로 볼 수 있다. 첫번째 term은 attractive force이고 두번째 term은 repulsive force이다. 이런 push & pull process를 통해 umap은 tsne보다 data의 overall structure를 잘 보존한다. (tsne는 기존에 가깝게 있던것들은 저차원에서도 가깝게 유지하지만, 기존에 멀리 있던것들은 저차원에서 가깝게 있더라도 penalty를 주지 않는다. 이에대한 설명은 마지막 참고 링크에 상세히 설명돼있다.)

![](https://miro.medium.com/max/1400/1*lfj7YbhgBHPs1MuX39w8yA.png)

이를 y_i에 대해 미분한뒤 stochastic gradient descent를 통해 update를 한다.

마지막으로 고려할 점은 high dimension에서 k의 역활을 할 수 있는 hyper parameter min_dist이다. 이점은 아직까지 햇갈리면서도 신기한점이다. 위에서 k의 역활은 사실상 $r$과 같다. $r$ 은 high dimension에서 k번째 neighbor와의 거리 / min_dist는 low dimension 에서 가장 가까운 점과의 거리이다. k는 고차원에서 local/global sturcture의 정도를 결정하는 한다고 볼 수 있다. 그렇다면 min_dist는 어떤 역활을 할까?

![](https://miro.medium.com/max/1094/1*i_CxdtZ8_nwDB_QsTM7okA.png)

![](https://miro.medium.com/max/1400/1*DiyDE0_oA8xtVzTKk8dxRw.png) 

위의 식은 low dimension에서 weight의 식이다. 여기서 a와 b는 min_dist에 의해 결정되는 값으로 $\approx$ 의 오른쪽 값을 근사할때 추정된다. 아래에서 볼 수 있듯이 min_dist는 고차원에서 $\rho$ 보다 작은 거리의 점들의 $p_{ij}$ 가 1로 만들어 주는 역활을 저차원에서 하고있다. 

![](https://miro.medium.com/max/1400/1*Nd0RUd5mY1J6EigvyE3TmA.png){: .align-center}



---

[https://umap-learn.readthedocs.io/en/latest/](https://umap-learn.readthedocs.io/en/latest/)

[https://www.youtube.com/watch?v=G9s3cE8TNZo&t=5484s](https://www.youtube.com/watch?v=G9s3cE8TNZo&t=5484s)

[https://www.youtube.com/watch?v=nq6iPZVUxZU](https://www.youtube.com/watch?v=nq6iPZVUxZU)

[https://towardsdatascience.com/how-exactly-umap-works-13e3040e1668+&cd=12&hl=ko&ct=clnk&gl=kr&client=safari](https://towardsdatascience.com/how-exactly-umap-works-13e3040e1668)